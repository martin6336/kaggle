stacking方法关键思想就是利用前一层模型的结果作为后面层的feature。二分类问题中可以直接拿预测出来是0还是1当新的feature，多分类问题讲道理也可以这么搞。
不要感觉怪，又不是去回归得出1，2,3，直观来讲就是某个样本的某个feature是0，1，2，3那他是对应类别的概率当然很大，不要被你那种回归的想法制约。**分类器预测的不是1，2，3，他预测出来的是属于各个类别的概率**
注意stacking其实就是给模型换一些浓缩的feature，所以这个feature不是一定只有一个（类别），也可以是多列像各个类别的概率。并且也可以添加其他天然的feature，这个就仁者见仁了。
