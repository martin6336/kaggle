数据处理
本文尝试了两种处理缺失值的方法。
第一种：如果某列缺失值达到了该列总数的一半就将该列删除。对于特征是分类性特征的情况，使用python中的map函数映射为整型数字，
便于机器处理。但是这种处理方式得到的数据应用到模型的效果并不好。
第二种：如果某列含有缺失值就将该列删除，并且将得到的新数据标准化会对提高模型精度作出贡献。本文采取的就是第二种方法。


变量生成

模型算法
本文总体思路是使用stacking思想融合多个模型，以达到提高模型精度的目标。

对于分类问题，最好的模型是各种分类树，当然也可以使用神经网络但是效果一般并不如决策树。
本文使用了多种决策数模型：俄罗斯一家公司开源的catboost，微软开源的lightgbm，和陈天奇大神开发的xgboost，以及正则化贪婪树regularized greedy forest（RGF），sklearn中的extratree，random forest tree除此之外还使用keras包实现神经网络。经多次实验可得logistic，extratree，randomtree以及神经网络的效果并不好。本地切分出验证集catboost，lightgbm，xgboost，RGF四种模型都可以达到0.8以上。为了进一步稳定预测概率并且提高预测精度，决定使用融合模型为logistic的stacking方法。

stacking方法的思想很简单。各个模型预测出来的概率作为logistic回归的自变量和样本的标签作为因变量。分别预测得到概率之后使用logistic融合各个概率得到最终结果。但是注意为了避免过拟合，对训练集做预测的时候采取5-fold交叉预测方法，保证预测的数据没有参与到模型的拟合中。

本文还尝试在训练模型的时候采用sklearn包中提供的calibration方法。这种方法简单的理解就是使预测出来的概率更接近真实的概率。但是这种方法对于本文的模型并没有明显的提高，并且由于使用calibration很费时间，所以本文最终并没有采用这种方法。但是代码中还是提供了使用calibation的选项。

并且本次机器学习比赛由于时间紧促，所以对单个模型并没有花费很多时间进行调参。调参有两种方法一种是人工调参，还有一种是使用hyperopt包，给出参数的范围后使用该包可以实现自动调参，但是效果一般并不如人工调参。本文代码的各个模型中都提供了使用hyperopt调参的选项。

代码结构
本文代码分为两大类，一类为model，另一类为utils。在model中可以进行调参，对本地数据进行划分验证集得到模型表现得分，还设置有calibration选项。
loacal文件将本地训练集按照4：1的比例划分为训练集和验证集，并运行各个模型方便观察模型表现。运行main文件可以得到最终提交的概率值。

其他讨论
模型融合本文还尝试过使用加权的方法，但是效果并不如使用logistic。并且由于时间原因，本文选用的模型并不多，尝试了使用knn尝试了不同参数，分别设置邻居数为2，4，8。并在融合阶段加入了各个样本距离最近邻居的距离，分别设置参数为1，2，4个邻居
