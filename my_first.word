数据处理
本文尝试了两种处理缺失值的方法。
第一种：如果某列缺失值达到了该列总数的一半就将该列删除。对于特征是分类性特征的情况，使用python中的map函数映射为整型数字，
便于机器处理。但是这种处理方式得到的数据应用到模型的效果并不好。
第二种：如果某列含有缺失值就将该列删除，并且将得到的新数据标准化会对提高模型精度作出贡献。本文采取的就是第二种方法。


变量生成
本文还构造了一些变量比如每个样本各个变量为0的个数，为1的个数，最值出现在哪个特征等等，但是模型的效果并不好。
模型算法
本文总体思路是使用stacking思想融合多个模型，以达到提高模型精度的目标。

对于分类问题，最好的模型是各种分类树，当然也可以使用神经网络但是效果一般并不如决策树。
本文使用了多种决策数模型：俄罗斯一家公司开源的catboost，微软开源的lightgbm，和陈天奇大神开发的xgboost，以及正则化贪婪树regularized greedy forest（RGF），sklearn中的extratree，random forest tree除此之外还使用keras包实现神经网络。经多次实验可得logistic，extratree，randomtree以及神经网络的效果并不好。本地切分出验证集catboost，lightgbm，xgboost，RGF四种模型都可以达到0.8以上。为了进一步稳定预测概率并且提高预测精度，决定使用融合模型为logistic的stacking方法。

stacking方法的思想很简单。各个模型预测出来的概率作为logistic回归的自变量和样本的标签作为因变量。分别预测得到概率之后使用logistic融合各个概率得到最终结果。但是注意为了避免过拟合，对训练集做预测的时候采取5-fold交叉预测方法，保证预测的数据没有参与到模型的拟合中。

本文还尝试在训练模型的时候采用sklearn包中提供的calibration方法。这种方法简单的理解就是使预测出来的概率更接近真实的概率。但是这种方法对于本文的模型并没有明显的提高，并且由于使用calibration很费时间，所以本文最终并没有采用这种方法。但是代码中还是提供了使用calibation的选项。

并且本次机器学习比赛由于时间紧促，所以对单个模型并没有花费很多时间进行调参。调参有两种方法一种是人工调参，还有一种是使用hyperopt包，给出参数的范围后使用该包可以实现自动调参，但是效果一般并不如人工调参。本文代码的各个模型中都提供了使用hyperopt调参的选项。

除此之外，本文还使用了knn算法邻居数分别设置为2，4，8，16，32，64，128。knn算法本身精度并不高，但是引入模型融合中，可以将模型精度进一步提高，虽然效果并不好。但是为了使模型更加稳定，本文还是将这些模型引入。除此之外，本文还在模型融合过程中引入了其他特征，就是样本中各个点距离最近的邻居的距离。本文只引入了三个距离特征，分别为与1，2，4个最近邻居的距离。在logistic模型中引入这三个变量也可以进一步提高模型精度。

代码结构
本文代码分为两大类，一类为model，另一类为utils。在model中可以进行调参，对本地数据进行划分验证集得到模型表现得分，还设置有calibration选项。
loacal文件将本地训练集按照4：1的比例划分为训练集和验证集，并运行各个模型方便观察模型表现。运行main文件可以得到最终提交的概率值。

其他讨论
模型融合本文还尝试过使用加权的方法，但是效果并不如使用logistic。并且对于定性变量，本文只是简单的将其替换为定量变量方便机器处理。其实这样是并不科学的。catboost中可以设置哪些变量为定性变量，这样应该可以进一步提高精度。但是本文限于时间原因没有进一步尝试。
